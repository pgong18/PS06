---
title: "STAT/MATH 495: Problem Set 06"
author: "Pei Gong"
date: "2017-10-17"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

# Load packages
library(tidyverse)
library(broom)
library(knitr)
```





# Collaboration

Please indicate who you collaborated with on this assignment: 


# Setup

Define truth, which again we know for the purposes of this assignment, but in
practice we won't:

* the true function f(x) i.e. the signal
* the true epsilon i.e. the noise, which in this case is Normal$(0, sd=\sigma)$.
Hence the standard deviation $\sigma$ determines the amount of noise.

```{r}
f <- function(x) {
  x^2
}
sigma <- 0.3
```

This is the target point we'll be trying to predict: $(0.95, f(0.95)) = (0.95, 0.95^2) = (0.95, 0.9025)$, Thus, the test set is just `x=0.95`

```{r}
x0 <- 0.95
y0<-x0^2
test_set <- data_frame(x=x0)
```

This function generates a random sample of size $n$; think of this as a "get new
data" function. Random in terms of both:

* (New) the predictor x (uniform on [0,1])
* the amount of noise $\epsilon$

```{r}
generate_sample <- function(f, n, sigma) {
  sample <- data_frame(
    x = runif(n = n, min = 0, max = 1),
    f_x = f(x),
    epsilon = rnorm(n = n, mean = 0, sd = sigma),
    y = f_x + epsilon
  )
  # Recall: We don't observe f(x) and epsilon, just (x, y)
  sample <- sample %>% 
    select(x, y)
  return(sample)
}
```


Define

* The number $n$ of observations $(x_i, y_i)$ in each sample. In the handout,
$n=100$ to keep plots uncrowded. Here we boost to $n=500$
* Number of samples of size $n$ to consider

```{r}
n <- 500
n_sample <- 10000
```

# Computation

```{r}
sample<-generate_sample(f,n,sigma)
model<-smooth.spline(sample$x,sample$y,df=2)
SE<-(predict(x=x0,model)$y-y0)^2
```


```{r}
predict_y<-runif(n = n_sample, min = 1, max = 2)
SE<-runif(n = n_sample, min = 1, max = 2)
```


```{r}
calculate<-function(n_sample,df){
   for (i in c(1:n_sample)){
    sample_single<-generate_sample(f,n,sigma)
    model<-smooth.spline(sample_single$x,sample_single$y,df=2)
    predict_y[i]<-predict(x=x0,model)$y
    y_real<-rnorm(1,y0,sigma)
    SE[i]<-(predict(x=x0,model)$y-y_real)^2
   }
    MSE_total<-mean(SE)
    variance_total<-var(predict_y)
    bias_squared_total<-(y0-mean(predict_y))^2
  return(c(MSE_total,variance_total,bias_squared_total))
}


```
    

# Tables

As done in Lec 2.7, for both

* An `lm` regression AKA a `smooth.spline(x, y, df=2)` model fit 
* A `smooth.spline(x, y, df=99)` model fit 

output tables comparing:

|  MSE| bias_squared|   var| irreducible|   sum|
|----:|------------:|-----:|-----------:|-----:|
|     X|           X  |     X |      X |         X |

where `sum = bias_squared + var + irreducible`. You can created cleanly formatted tables like the one above by piping a data frame into `knitr::kable(digits=4)`.

df=2

```{r}
predictions_2<-calculate(n_sample,2);predictions_2
sum_2<-predictions_2[2]+predictions_2[3]+sigma^2;sum_2
```

|  MSE| bias_squared|   var| irreducible|   sum|
|----:|------------:|-----:|-----------:|-----:|
|    0.106028594 |  0.013885 | 0.000672| 0.09 | 0.1045578|

df=99

```{r}
predictions_99<-calculate(n_sample,99);predictions_99
sum_99<-predictions_99[2]+predictions_99[3]+sigma^2;sum_99
```

|  MSE| bias_squared|   var| irreducible|   sum|
|----:|------------:|-----:|-----------:|-----:|
|    0.105969 |  0.013843 | 0.000665| 0.09 |  0.1045081|


# Analysis

**Questions**:

1. Based on the topics covered in Lec 2.7, name one possible "sanity check" for your results. Name another if you can.
1. In **two** sentences or less, give a rough sketch of what the procedure would
be to get the breakdown of $$\mbox{MSE}\left[\widehat{f}(x)\right]$$ for *all*
$x$ in this example, and not just for $$\mbox{MSE}\left[\widehat{f}(x_0)\right]
= \mbox{MSE}\left[\widehat{f}(0.95)\right]$$.
1. Which of the two models would you choose for predicting the point of interest and why?

**Answers**:

1.
1.
1.
